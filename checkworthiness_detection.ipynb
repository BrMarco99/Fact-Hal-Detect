{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPZLZgfkUbZUsl9JwPdD5D7"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"J4BtavEFbWhT","executionInfo":{"status":"ok","timestamp":1720449868318,"user_tz":-120,"elapsed":32226,"user":{"displayName":"Marco","userId":"18207980685138219401"}},"outputId":"ef3a7ceb-4963-47f7-829a-9f5cee6dac49"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","import os\n","os.environ['CUDA_VISIBLE_DEVICES'] = \"0\"\n","\n","import json\n","from time import time\n","import locale\n","import re\n","import numpy as np\n","locale.setlocale(locale.LC_ALL, 'en_US.UTF-8')\n","drive.mount('/content/drive')\n","os.chdir('/content/drive/MyDrive/Tesi/Codice/FINAL')\n","\n","\n","\n","import requests\n","import json\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import scipy.stats\n","import seaborn as sns\n","from pylab import rcParams\n","\n","!pip -q install jsonlines"]},{"cell_type":"code","source":["#EXECUTE THIS CELL ONLY WHEN LLM IS NECESSARY\n","#huggingface login\n","from huggingface_hub import login\n","hf_auth = \"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\"\n","login(token=hf_auth)\n","!pip install -q torch numpy transformers pandas tqdm accelerate sentence-transformers setGPU\n","!pip -q install bitsandbytes backoff"],"metadata":{"id":"HHbSHrUpbdLN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Manual Review"],"metadata":{"id":"Oelo2wlahOMT"}},{"cell_type":"markdown","source":["Create the reduced dataset from TruthfulQA for the manual review"],"metadata":{"id":"NosZ_6vafMNu"}},{"cell_type":"code","source":["os.chdir('/content/drive/MyDrive/Tesi/Codice/FINAL/files')\n","import copy, csv, jsonlines\n","def getList(input):\n","  sentences = input.split(';')\n","  sentences = [sentence.strip() for sentence in sentences]\n","  return sentences\n","\n","with open(\"truthful_qa.csv\", \"r\", encoding=\"utf-8\") as csvfile:\n","  reader = csv.DictReader(csvfile)\n","  with jsonlines.open(\"reduced_for_review_truthful_qa.jsonl\", \"w\") as writer:\n","    for line in reader:\n","\n","      new_data = dict()\n","      new_data[\"Type\"] = line[\"\\ufeffType\"]\n","      new_data[\"Category\"] = line[\"Category\"]\n","      new_data[\"Question\"] = line[\"Question\"]\n","      new_data[\"Source\"] = line[\"Source\"]\n","\n","      #get only the first two answers (whenever possible)\n","      c_answers = getList(line[\"Correct Answers\"])\n","      count = 0\n","      for answer in c_answers:\n","        if count != 2:\n","          final_data = copy.deepcopy(new_data)\n","          final_data[\"Answer\"] = answer\n","          final_data[\"Real_label\"] = True\n","          writer.write(final_data)\n","          count = count + 1\n","        else:\n","          break\n","\n","      #get only the first two answers (whenever possible)\n","      i_answers = getList(line[\"Incorrect Answers\"])\n","      count = 0\n","      for answer in i_answers:\n","        if count != 2:\n","          final_data = copy.deepcopy(new_data)\n","          final_data[\"Answer\"] = answer\n","          final_data[\"Real_label\"] = False\n","          writer.write(final_data)\n","          count = count + 1\n","        else:\n","          break\n"],"metadata":{"id":"KXdy-1lgdTSS","executionInfo":{"status":"ok","timestamp":1720444693545,"user_tz":-120,"elapsed":365,"user":{"displayName":"Marco","userId":"18207980685138219401"}}},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":["Add the ids from the original and complete TruthfulQA dataset (see the other notebook for the original file creation)"],"metadata":{"id":"LabcpPL4fXmU"}},{"cell_type":"code","source":["os.chdir('/content/drive/MyDrive/Tesi/Codice/FINAL/files')\n","df1 = pd.read_json('WITH_EV_truthful_qa_UNROLLED.jsonl', lines = True)\n","df2 = pd.read_json('reduced_for_review_truthful_qa.jsonl', lines = True)\n","# Create a mapping dictionary from df1's question-answer pairs to IDs\n","mapping = df1.groupby(['Question', 'Answer'])['ID'].max().to_dict()\n","# Apply the mapping to df2\n","df2['ID'] = df2.apply(lambda row: mapping[(row['Question'], row['Answer'])], axis=1)\n","df2 = df2.rename(columns={'Real_label': 'Factuality_ground_label'})\n","#reorder the columns\n","desired_columns = ['ID', 'Type', 'Category', 'Question', 'Answer', 'Factuality_ground_label', 'Source']\n","df2 = df2[desired_columns]\n","df2.to_json('reduced_for_review_truthful_qa.jsonl', orient='records', lines=True)\n"],"metadata":{"id":"B6vVWvo2ffK9","executionInfo":{"status":"ok","timestamp":1720444940912,"user_tz":-120,"elapsed":1410,"user":{"displayName":"Marco","userId":"18207980685138219401"}}},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":["Test prompts on manually reviewed dataset\n"],"metadata":{"id":"CaszdvRBjwhX"}},{"cell_type":"code","source":["#LONGPROMPT\n","from sklearn.metrics import accuracy_score\n","\n","os.chdir('/content/drive/MyDrive/Tesi/Codice/FINAL/src')\n","!python qa_relevance.py --model 'Starling-LM-7B-alpha' --input 'manual_checkworthiness_dataset.jsonl' --output 'longprompt_checkworthiness_dataset.jsonl' --prompt 'checkworthiness_LongPrompt.txt'--resume\n","os.chdir('/content/drive/MyDrive/Tesi/Codice/FINAL/files/checkworthiness_detection')\n","df = pd.read_json(\"longprompt_checkworthiness_dataset.jsonl\", lines = True)\n","manual = df['manual_checkworthy'].astype(int)\n","model = df['model_checkworthiness'].astype(int)\n","print(f'accuracy: {accuracy_score(manual, model):.4f}')\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CHK6xqTKigQr","executionInfo":{"status":"ok","timestamp":1720447332353,"user_tz":-120,"elapsed":351,"user":{"displayName":"Marco","userId":"18207980685138219401"}},"outputId":"dbea5060-03a6-49ed-a322-f0d99fe6dd21"},"execution_count":24,"outputs":[{"output_type":"stream","name":"stdout","text":["accuracy: 0.7653\n"]}]},{"cell_type":"code","source":["#SHORTPROMPT\n","from sklearn.metrics import accuracy_score\n","\n","os.chdir('/content/drive/MyDrive/Tesi/Codice/FINAL/src')\n","!python qa_relevance.py --model 'Starling-LM-7B-alpha' --input 'manual_checkworthiness_dataset.jsonl' --output 'shortprompt_checkworthiness_dataset.jsonl' --prompt 'checkworthiness_ShortPrompt.txt'--resume\n","os.chdir('/content/drive/MyDrive/Tesi/Codice/FINAL/files/checkworthiness_detection')\n","df = pd.read_json(\"shortprompt_checkworthiness_dataset.jsonl\", lines = True)\n","manual = df['manual_checkworthy'].astype(int)\n","model = df['model_checkworthiness'].astype(int)\n","print(f'accuracy: {accuracy_score(manual, model):.4f}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UevVjSW2nVOA","executionInfo":{"status":"ok","timestamp":1720447406800,"user_tz":-120,"elapsed":334,"user":{"displayName":"Marco","userId":"18207980685138219401"}},"outputId":"65292cd7-3097-4478-b777-696d66305d53"},"execution_count":26,"outputs":[{"output_type":"stream","name":"stdout","text":["accuracy: 0.8367\n"]}]},{"cell_type":"markdown","source":["#Automatic Checkworthiness detection"],"metadata":{"id":"oZfmyFFOr4TV"}},{"cell_type":"markdown","metadata":{"id":"dQY35VGGkH-K"},"source":["Filter the problematic samples (Disputable categories, \"nothing, many, few\")"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"sWy98cqJR0dQ"},"outputs":[],"source":["os.chdir('/content/drive/MyDrive/Tesi/Codice/FINAL/files')\n","df = pd.read_json(\"truthful_qa_UNROLLED.jsonl\", lines = True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7eOQC8VGR3du"},"outputs":[],"source":["categories_to_exclude = ['Myths and Fairytales', 'Paranormal', 'Superstitions', 'Religion', 'Fiction', 'Advertising', 'Indexical Error: Identity']\n","# Define words to exclude\n","words_to_exclude = [\"many\", \"few\", \"nothing\", \"ultimate\"]\n","\n","# Create a regex pattern to match any of the words\n","pattern = '|'.join(words_to_exclude)\n","\n","# Filter DataFrame to exclude rows where 'Answer' contains any of the specified words (case insensitive)\n","filtered_df = df[~df['Answer'].str.contains(pattern, case=False, regex=True) & ~df['Category'].isin(categories_to_exclude)]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1RuzZNNxR3YZ"},"outputs":[],"source":["filtered_df.to_json('FILTERED_truthful_qa_UNROLLED.jsonl', orient='records', lines=True)"]},{"cell_type":"markdown","source":["Create Checkworthiness dataset (ShortPrompt)"],"metadata":{"id":"qSMRFjg6sKPJ"}},{"cell_type":"code","source":["os.chdir('/content/drive/MyDrive/Tesi/Codice/FINAL/files')\n","filtered_df = pd.read_json(\"FILTERED_truthful_qa_UNROLLED.jsonl\", lines = True)"],"metadata":{"id":"4cQSsrhKsKiK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["os.chdir('/content/drive/MyDrive/Tesi/Codice/FINAL/src')\n","!python qa_relevance.py --model 'Starling-LM-7B-alpha' --input 'FILTERED_truthful_qa_UNROLLED.jsonl' --output 'or_model_checkworthiness_dataset.jsonl' --prompt 'checkworthiness_ShortPrompt.txt' --resume"],"metadata":{"id":"D-pVoGJusZwX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Modify the already reviewed samples with the labels manually obtained"],"metadata":{"id":"xooeDh5TtUAk"}},{"cell_type":"code","source":["import pandas as pd\n","\n","os.chdir('/content/drive/MyDrive/Tesi/Codice/FINAL/files/checkworthiness_detection')\n","df1 = pd.read_json(\"or_model_checkworthiness_dataset.jsonl\", lines = True)\n","df2 = pd.read_json(\"manual_checkworthiness_dataset.jsonl\", lines = True)\n","df2 = df2[[\"ID\", \"manual_checkworthy\"]]\n","# Merge DataFrames on 'ID'\n","merged_df = pd.merge(df1, df2, on='ID', how='left')\n","\n","# Update 'model_label' with 'manual_label' where applicable\n","merged_df['Model_checkworthiness_label'] = merged_df['manual_checkworthy'].combine_first(merged_df['Model_checkworthiness_label'])\n","\n","# Drop the 'manual_label' column as it's no longer needed\n","updated_df1 = merged_df.drop(columns=['manual_checkworthy'])\n","\n","#updated_df1.to_json('model_checkworthiness_dataset.jsonl', orient='records', lines=True)"],"metadata":{"id":"nhlJ7u8fs0Yt","executionInfo":{"status":"ok","timestamp":1720449869624,"user_tz":-120,"elapsed":1310,"user":{"displayName":"Marco","userId":"18207980685138219401"}}},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":["Select only the checkworthy samples"],"metadata":{"id":"j1am5_kowKQH"}},{"cell_type":"code","source":["os.chdir('/content/drive/MyDrive/Tesi/Codice/FINAL/files/checkworthiness_detection')\n","df1 = pd.read_json(\"model_checkworthiness_dataset.jsonl\", lines = True)\n","check_df = df1[df1['Model_checkworthiness_label'] == True]\n","check_df.to_json('checkworthy_samples_dataset.jsonl', orient='records', lines=True)\n","len(check_df)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BEdZm5v4wDkP","executionInfo":{"status":"ok","timestamp":1720449922687,"user_tz":-120,"elapsed":1259,"user":{"displayName":"Marco","userId":"18207980685138219401"}},"outputId":"0c369cc1-a9cb-4cba-fd45-8f0fef418e2b"},"execution_count":6,"outputs":[{"output_type":"execute_result","data":{"text/plain":["3944"]},"metadata":{},"execution_count":6}]},{"cell_type":"code","source":[],"metadata":{"id":"9L4wtddZzm9-"},"execution_count":null,"outputs":[]}]}