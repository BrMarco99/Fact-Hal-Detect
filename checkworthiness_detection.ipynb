{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 32226,
     "status": "ok",
     "timestamp": 1720449868318,
     "user": {
      "displayName": "Marco",
      "userId": "18207980685138219401"
     },
     "user_tz": -120
    },
    "id": "J4BtavEFbWhT",
    "outputId": "ef3a7ceb-4963-47f7-829a-9f5cee6dac49"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"0\"\n",
    "\n",
    "import json\n",
    "from time import time\n",
    "import locale\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "HHbSHrUpbdLN"
   },
   "outputs": [],
   "source": [
    "files_path = \"files\"\n",
    "TQA_files_path = os.path.join(files_path, \"TruthfulQA\")\n",
    "output_files = \"out_files_temp\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Oelo2wlahOMT"
   },
   "source": [
    "# Manual Review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NosZ_6vafMNu"
   },
   "source": [
    "Create the reduced dataset from TruthfulQA for the manual review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 365,
     "status": "ok",
     "timestamp": 1720444693545,
     "user": {
      "displayName": "Marco",
      "userId": "18207980685138219401"
     },
     "user_tz": -120
    },
    "id": "KXdy-1lgdTSS"
   },
   "outputs": [],
   "source": [
    "import copy, csv, jsonlines\n",
    "def getList(input):\n",
    "  sentences = input.split(';')\n",
    "  sentences = [sentence.strip() for sentence in sentences]\n",
    "  return sentences\n",
    "\n",
    "with open(os.path.join(TQA_files_path, \"truthful_qa.csv\"), \"r\", encoding=\"utf-8\") as csvfile:\n",
    "  reader = csv.DictReader(csvfile)\n",
    "  with jsonlines.open(os.path.join(output_files, \"reduced_for_review_truthful_qa.jsonl\"), \"w\") as writer:\n",
    "    for line in reader:\n",
    "\n",
    "      new_data = dict()\n",
    "      new_data[\"Type\"] = line[\"\\ufeffType\"]\n",
    "      new_data[\"Category\"] = line[\"Category\"]\n",
    "      new_data[\"Question\"] = line[\"Question\"]\n",
    "      new_data[\"Source\"] = line[\"Source\"]\n",
    "\n",
    "      #get only the first two answers (whenever possible)\n",
    "      c_answers = getList(line[\"Correct Answers\"])\n",
    "      count = 0\n",
    "      for answer in c_answers:\n",
    "        if count != 2:\n",
    "          final_data = copy.deepcopy(new_data)\n",
    "          final_data[\"Answer\"] = answer\n",
    "          final_data[\"Real_label\"] = True\n",
    "          writer.write(final_data)\n",
    "          count = count + 1\n",
    "        else:\n",
    "          break\n",
    "\n",
    "      #get only the first two answers (whenever possible)\n",
    "      i_answers = getList(line[\"Incorrect Answers\"])\n",
    "      count = 0\n",
    "      for answer in i_answers:\n",
    "        if count != 2:\n",
    "          final_data = copy.deepcopy(new_data)\n",
    "          final_data[\"Answer\"] = answer\n",
    "          final_data[\"Real_label\"] = False\n",
    "          writer.write(final_data)\n",
    "          count = count + 1\n",
    "        else:\n",
    "          break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LabcpPL4fXmU"
   },
   "source": [
    "Add the ids from the original and complete TruthfulQA dataset (see the other notebook for the original file creation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 1410,
     "status": "ok",
     "timestamp": 1720444940912,
     "user": {
      "displayName": "Marco",
      "userId": "18207980685138219401"
     },
     "user_tz": -120
    },
    "id": "B6vVWvo2ffK9"
   },
   "outputs": [],
   "source": [
    "df1 = pd.read_json(os.path.join(TQA_files_path, 'WITH_EV_truthful_qa_UNROLLED.jsonl'), lines = True)\n",
    "df2 = pd.read_json(os.path.join(output_files, \"reduced_for_review_truthful_qa.jsonl\"), lines = True)\n",
    "# Create a mapping dictionary from df1's question-answer pairs to IDs\n",
    "mapping = df1.groupby(['Question', 'Answer'])['ID'].max().to_dict()\n",
    "# Apply the mapping to df2\n",
    "df2['ID'] = df2.apply(lambda row: mapping[(row['Question'], row['Answer'])], axis=1)\n",
    "df2 = df2.rename(columns={'Real_label': 'Factuality_ground_label'})\n",
    "#reorder the columns\n",
    "desired_columns = ['ID', 'Type', 'Category', 'Question', 'Answer', 'Factuality_ground_label', 'Source']\n",
    "df2 = df2[desired_columns]\n",
    "df2.to_json(os.path.join(output_files, \"reduced_for_review_truthful_qa.jsonl\"), orient='records', lines=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CaszdvRBjwhX"
   },
   "source": [
    "Test prompts on manually reviewed dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 351,
     "status": "ok",
     "timestamp": 1720447332353,
     "user": {
      "displayName": "Marco",
      "userId": "18207980685138219401"
     },
     "user_tz": -120
    },
    "id": "CHK6xqTKigQr",
    "outputId": "dbea5060-03a6-49ed-a322-f0d99fe6dd21"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.7653\n"
     ]
    }
   ],
   "source": [
    "#LONGPROMPT\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "os.chdir('/content/drive/MyDrive/Tesi/Codice/FINAL/src')\n",
    "!python qa_relevance.py --model 'Starling-LM-7B-alpha' --input 'manual_checkworthiness_dataset.jsonl' --output 'longprompt_checkworthiness_dataset.jsonl' --prompt 'checkworthiness_LongPrompt.txt'--resume\n",
    "os.chdir('/content/drive/MyDrive/Tesi/Codice/FINAL/files/checkworthiness_detection')\n",
    "df = pd.read_json(\"longprompt_checkworthiness_dataset.jsonl\", lines = True)\n",
    "manual = df['manual_checkworthy'].astype(int)\n",
    "model = df['model_checkworthiness'].astype(int)\n",
    "print(f'accuracy: {accuracy_score(manual, model):.4f}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 334,
     "status": "ok",
     "timestamp": 1720447406800,
     "user": {
      "displayName": "Marco",
      "userId": "18207980685138219401"
     },
     "user_tz": -120
    },
    "id": "UevVjSW2nVOA",
    "outputId": "65292cd7-3097-4478-b777-696d66305d53"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.8367\n"
     ]
    }
   ],
   "source": [
    "#SHORTPROMPT\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "os.chdir('/content/drive/MyDrive/Tesi/Codice/FINAL/src')\n",
    "!python qa_relevance.py --model 'Starling-LM-7B-alpha' --input 'manual_checkworthiness_dataset.jsonl' --output 'shortprompt_checkworthiness_dataset.jsonl' --prompt 'checkworthiness_ShortPrompt.txt'--resume\n",
    "os.chdir('/content/drive/MyDrive/Tesi/Codice/FINAL/files/checkworthiness_detection')\n",
    "df = pd.read_json(\"shortprompt_checkworthiness_dataset.jsonl\", lines = True)\n",
    "manual = df['manual_checkworthy'].astype(int)\n",
    "model = df['model_checkworthiness'].astype(int)\n",
    "print(f'accuracy: {accuracy_score(manual, model):.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oZfmyFFOr4TV"
   },
   "source": [
    "#Automatic Checkworthiness detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dQY35VGGkH-K"
   },
   "source": [
    "Filter the problematic samples (Disputable categories, \"nothing, many, few\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "sWy98cqJR0dQ"
   },
   "outputs": [],
   "source": [
    "os.chdir('/content/drive/MyDrive/Tesi/Codice/FINAL/files')\n",
    "df = pd.read_json(\"truthful_qa_UNROLLED.jsonl\", lines = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7eOQC8VGR3du"
   },
   "outputs": [],
   "source": [
    "categories_to_exclude = ['Myths and Fairytales', 'Paranormal', 'Superstitions', 'Religion', 'Fiction', 'Advertising', 'Indexical Error: Identity']\n",
    "# Define words to exclude\n",
    "words_to_exclude = [\"many\", \"few\", \"nothing\", \"ultimate\"]\n",
    "\n",
    "# Create a regex pattern to match any of the words\n",
    "pattern = '|'.join(words_to_exclude)\n",
    "\n",
    "# Filter DataFrame to exclude rows where 'Answer' contains any of the specified words (case insensitive)\n",
    "filtered_df = df[~df['Answer'].str.contains(pattern, case=False, regex=True) & ~df['Category'].isin(categories_to_exclude)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1RuzZNNxR3YZ"
   },
   "outputs": [],
   "source": [
    "filtered_df.to_json('FILTERED_truthful_qa_UNROLLED.jsonl', orient='records', lines=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qSMRFjg6sKPJ"
   },
   "source": [
    "Create Checkworthiness dataset (ShortPrompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4cQSsrhKsKiK"
   },
   "outputs": [],
   "source": [
    "os.chdir('/content/drive/MyDrive/Tesi/Codice/FINAL/files')\n",
    "filtered_df = pd.read_json(\"FILTERED_truthful_qa_UNROLLED.jsonl\", lines = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D-pVoGJusZwX"
   },
   "outputs": [],
   "source": [
    "os.chdir('/content/drive/MyDrive/Tesi/Codice/FINAL/src')\n",
    "!python qa_relevance.py --model 'Starling-LM-7B-alpha' --input 'FILTERED_truthful_qa_UNROLLED.jsonl' --output 'or_model_checkworthiness_dataset.jsonl' --prompt 'checkworthiness_ShortPrompt.txt' --resume"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xooeDh5TtUAk"
   },
   "source": [
    "Modify the already reviewed samples with the labels manually obtained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 1310,
     "status": "ok",
     "timestamp": 1720449869624,
     "user": {
      "displayName": "Marco",
      "userId": "18207980685138219401"
     },
     "user_tz": -120
    },
    "id": "nhlJ7u8fs0Yt"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "os.chdir('/content/drive/MyDrive/Tesi/Codice/FINAL/files/checkworthiness_detection')\n",
    "df1 = pd.read_json(\"or_model_checkworthiness_dataset.jsonl\", lines = True)\n",
    "df2 = pd.read_json(\"manual_checkworthiness_dataset.jsonl\", lines = True)\n",
    "df2 = df2[[\"ID\", \"manual_checkworthy\"]]\n",
    "# Merge DataFrames on 'ID'\n",
    "merged_df = pd.merge(df1, df2, on='ID', how='left')\n",
    "\n",
    "# Update 'model_label' with 'manual_label' where applicable\n",
    "merged_df['Model_checkworthiness_label'] = merged_df['manual_checkworthy'].combine_first(merged_df['Model_checkworthiness_label'])\n",
    "\n",
    "# Drop the 'manual_label' column as it's no longer needed\n",
    "updated_df1 = merged_df.drop(columns=['manual_checkworthy'])\n",
    "\n",
    "#updated_df1.to_json('model_checkworthiness_dataset.jsonl', orient='records', lines=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j1am5_kowKQH"
   },
   "source": [
    "Select only the checkworthy samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1259,
     "status": "ok",
     "timestamp": 1720449922687,
     "user": {
      "displayName": "Marco",
      "userId": "18207980685138219401"
     },
     "user_tz": -120
    },
    "id": "BEdZm5v4wDkP",
    "outputId": "0c369cc1-a9cb-4cba-fd45-8f0fef418e2b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3944"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.chdir('/content/drive/MyDrive/Tesi/Codice/FINAL/files/checkworthiness_detection')\n",
    "df1 = pd.read_json(\"model_checkworthiness_dataset.jsonl\", lines = True)\n",
    "check_df = df1[df1['Model_checkworthiness_label'] == True]\n",
    "check_df.to_json('checkworthy_samples_dataset.jsonl', orient='records', lines=True)\n",
    "len(check_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9L4wtddZzm9-"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPZLZgfkUbZUsl9JwPdD5D7",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
