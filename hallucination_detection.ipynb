{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xrmsSdfjQi5M",
        "outputId": "7d0f2e97-f9b9-4552-b4a8-d9c2fa12b0e1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "os.environ['CUDA_VISIBLE_DEVICES'] = \"0\"\n",
        "\n",
        "import json\n",
        "from time import time\n",
        "import locale\n",
        "import re\n",
        "import numpy as np\n",
        "locale.setlocale(locale.LC_ALL, 'en_US.UTF-8')\n",
        "drive.mount('/content/drive')\n",
        "os.chdir('/content/drive/MyDrive/Tesi/Codice/FINAL')\n",
        "\n",
        "\n",
        "\n",
        "import requests\n",
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy.stats\n",
        "import seaborn as sns\n",
        "from pylab import rcParams\n",
        "\n",
        "!pip -q install jsonlines"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "86mMhdtACAiD"
      },
      "outputs": [],
      "source": [
        "#huggingface login\n",
        "from huggingface_hub import login\n",
        "hf_auth = \"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\"\n",
        "login(token=hf_auth)\n",
        "!pip install -q torch numpy transformers pandas tqdm accelerate sentence-transformers setGPU\n",
        "!pip -q install bitsandbytes backoff"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zIKjwn71V3yq"
      },
      "source": [
        "#Create COMPLETE dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l5NduusGV3UJ"
      },
      "outputs": [],
      "source": [
        "os.chdir('/content/drive/MyDrive/Tesi/Codice/FINAL/files')\n",
        "import copy, csv, jsonlines\n",
        "def getList(input):\n",
        "  sentences = input.split(';')\n",
        "  sentences = [sentence.strip() for sentence in sentences]\n",
        "  return sentences\n",
        "\n",
        "with open(\"truthful_qa.csv\", \"r\", encoding=\"utf-8\") as csvfile:\n",
        "  reader = csv.DictReader(csvfile)\n",
        "  with jsonlines.open(\"truthful_qa_UNROLLED.jsonl\", \"w\") as writer:\n",
        "    for line in reader:\n",
        "\n",
        "      new_data = dict()\n",
        "      new_data[\"Type\"] = line[\"\\ufeffType\"]\n",
        "      new_data[\"Category\"] = line[\"Category\"]\n",
        "      new_data[\"Question\"] = line[\"Question\"]\n",
        "      new_data[\"Source\"] = line[\"Source\"]\n",
        "\n",
        "      #get the answers\n",
        "      c_answers = getList(line[\"Correct Answers\"])\n",
        "      for answer in c_answers:\n",
        "        final_data = copy.deepcopy(new_data)\n",
        "        final_data[\"Answer\"] = answer\n",
        "        final_data[\"Real_label\"] = True\n",
        "        writer.write(final_data)\n",
        "\n",
        "      #get the answers\n",
        "      i_answers = getList(line[\"Incorrect Answers\"])\n",
        "      for answer in i_answers:\n",
        "        final_data = copy.deepcopy(new_data)\n",
        "        final_data[\"Answer\"] = answer\n",
        "        final_data[\"Real_label\"] = False\n",
        "        writer.write(final_data)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yV5pgPAoPRL7"
      },
      "source": [
        "Add IDs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3K4SDbbrV3E-"
      },
      "outputs": [],
      "source": [
        "os.chdir('/content/drive/MyDrive/Tesi/Codice/FINAL/files')\n",
        "df = pd.read_json(\"truthful_qa_UNROLLED.jsonl\", lines = True)\n",
        "df['ID'] = range(0, len(df))\n",
        "df = df.rename(columns={'Real_label': 'Factuality_ground_label'})\n",
        "new_order = ['ID', 'Type', 'Category', 'Question', 'Answer', 'Source', 'Factuality_ground_label']\n",
        "df = df[new_order]\n",
        "df.to_json('truthful_qa_UNROLLED.jsonl', orient='records', lines=True)\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C5Mp638pTlGE"
      },
      "source": [
        "#Retrieve evidence for every sample of the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "poTaKGIMCDvM"
      },
      "outputs": [],
      "source": [
        "os.chdir('/content/drive/MyDrive/Tesi/Codice/FINAL/src')\n",
        "!python retrieve_from_link.py --input 'truthful_qa_UNROLLED.jsonl' --output 'WITH_EV_truthful_qa_UNROLLED.jsonl' --resume"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BdAvMZt8UPqq"
      },
      "source": [
        "#Exclude the samples with no evidence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hcs15Neh9A9I"
      },
      "outputs": [],
      "source": [
        "os.chdir('/content/drive/MyDrive/Tesi/Codice/FINAL/files')\n",
        "df = pd.read_json('WITH_EV_truthful_qa_UNROLLED.jsonl', lines = True)\n",
        "filtered_df = df[df[\"Evidences\"].apply(len) == 0]\n",
        "remaining_df = df[~df.index.isin(filtered_df.index)]\n",
        "remaining_df.to_json('WITH_EV_truthful_qa_EXCLUDE_NO_EV.jsonl', orient='records', lines=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w278fYbgl5ug"
      },
      "source": [
        "#Shuffle the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fJQFovitmA6Y"
      },
      "outputs": [],
      "source": [
        "os.chdir('/content/drive/MyDrive/Tesi/Codice/FINAL/files')\n",
        "df = pd.read_json('WITH_EV_truthful_qa_EXCLUDE_NO_EV.jsonl', lines = True)\n",
        "shuffled_df = df.sample(frac=1).reset_index(drop=True)\n",
        "shuffled_df.to_json('SHUFFLED_WITH_EV_truthful_qa_EXCLUDE_NO_EV.jsonl', orient='records', lines=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qaIGNXQ-XQaV"
      },
      "source": [
        "#Internal A (Question + Answer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Wo6s8PHXSJr"
      },
      "outputs": [],
      "source": [
        "os.chdir('/content/drive/MyDrive/Tesi/Codice/FINAL/src')\n",
        "!python hal_detect_internal_A.py --input 'SHUFFLED_WITH_EV_truthful_qa_EXCLUDE_NO_EV.jsonl' --output 'truthful_qa_Internal_A_states.jsonl' --resume"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xukiUAe5gaEh"
      },
      "source": [
        "#Internal B (Question + Answer + Knowledge)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jGu7XEOaget1"
      },
      "outputs": [],
      "source": [
        "os.chdir('/content/drive/MyDrive/Tesi/Codice/FINAL/src')\n",
        "!python hal_detect_internal_B.py --input 'SHUFFLED_WITH_EV_truthful_qa_EXCLUDE_NO_EV.jsonl' --output 'truthful_qa_Internal_B_states.jsonl' --resume"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vVarnVnx_X3j"
      },
      "source": [
        "# Internal C (Knowledge + Question + Answer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KJuCp86C_dY0"
      },
      "outputs": [],
      "source": [
        "os.chdir('/content/drive/MyDrive/Tesi/Codice/FINAL/src')\n",
        "!python hal_detect_internal_C.py --input 'SHUFFLED_WITH_EV_truthful_qa_EXCLUDE_NO_EV.jsonl' --output 'truthful_qa_Internal_C_states.jsonl' --resume"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YRgiHsw7gAEZ"
      },
      "source": [
        "#Retrieve the relevant layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FxVd-WJPgCkT"
      },
      "outputs": [],
      "source": [
        "import jsonlines\n",
        "def retrieveLayer(number, letter):\n",
        "  os.chdir(f'/content/drive/MyDrive/Tesi/Codice/FINAL/files/Internal_{letter}/')\n",
        "  inputName = f\"truthful_qa_Internal_{letter}_states.jsonl\"\n",
        "  outputName = f\"Internal_{letter}_layer_{number}_th.jsonl\"\n",
        "  with jsonlines.open(inputName, \"r\") as f:\n",
        "    with jsonlines.open(outputName, \"w\") as writer:\n",
        "      for line in f:\n",
        "        new_data = dict()\n",
        "        new_data[\"ID\"] = line[\"ID\"]\n",
        "        new_data[\"Factuality_ground_label\"] = line[\"Factuality_ground_label\"]\n",
        "        new_data[\"hidden_states\"] = line [\"hidden_states\"][number]\n",
        "        writer.write(new_data)\n",
        "\n",
        "  #CHECK\n",
        "  with jsonlines.open(outputName, \"r\") as f:\n",
        "    i = 0\n",
        "    for line in f:\n",
        "      i = i + 1\n",
        "    print(\"Samples in file:\", i)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JNZSwL6vjcmz",
        "outputId": "3ff96473-a515-402c-efcf-63ad6019d47b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Samples in file: 5374\n"
          ]
        }
      ],
      "source": [
        "retrieveLayer(20, \"A\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4P7dJUUyQScg"
      },
      "source": [
        "#Hold-Out + Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MwpXhJ95REPU"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import pandas as pd\n",
        "from sklearn.metrics import accuracy_score\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn import datasets\n",
        "import pickle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UdWihjFbPfVy"
      },
      "outputs": [],
      "source": [
        "def hold_out_classifier(number, letter, dataset_name, filtered):\n",
        "\n",
        "  os.chdir(f'/content/drive/MyDrive/Tesi/Codice/FINAL/files/Internal_{letter}/')\n",
        "  fileDataName = f\"Internal_{letter}_layer_{number}_th.jsonl\"\n",
        "  df = pd.read_json(fileDataName, lines=True)\n",
        "\n",
        "  if filtered:\n",
        "    os.chdir(f'/content/drive/MyDrive/Tesi/Codice/FINAL/files/')\n",
        "    df2 = pd.read_json('checkworthy_samples_dataset_WITH_EV.jsonl', lines=True)\n",
        "    ids = df2['ID']\n",
        "    #get the subset of the dataframe\n",
        "    filtered_df = df[df['ID'].isin(list(ids))]\n",
        "    df = filtered_df\n",
        "\n",
        "  h_states_x = df['hidden_states'].tolist()\n",
        "  labels_y = df[\"Factuality_ground_label\"].tolist()\n",
        "  ids = df[\"ID\"].tolist()  # Assuming the ID field is named \"ID\"\n",
        "  train_x, test_x, train_y, test_y, train_ids, test_ids = train_test_split(h_states_x, labels_y, ids, test_size=0.2, random_state=2307)\n",
        "\n",
        "    # Print lengths of train and test sets\n",
        "  name = f\"{dataset_name}_Internal_{letter}_{number}_layer\"\n",
        "  print(name.upper())\n",
        "  print(\"Train set size:\", len(train_x))\n",
        "  print(\"Test set size:\", len(test_x))\n",
        "\n",
        "\n",
        "  #fit model and predict\n",
        "  model = LogisticRegression(solver='lbfgs', max_iter=3000).fit(train_x, train_y)\n",
        "  pred_y = model.predict(test_x)\n",
        "  #accuracy\n",
        "  print(f'accuracy: {accuracy_score(test_y, pred_y):.4f}')\n",
        "  #confusion matrix\n",
        "  cm = confusion_matrix(test_y, pred_y, labels=[True, False])\n",
        "  disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[True, False])\n",
        "  disp.plot()\n",
        "  plt.show()\n",
        "  #classification report\n",
        "  print('\\nclassification report:\\n')\n",
        "  print(classification_report(test_y, pred_y, target_names=[\"True\",\"False\"]))\n",
        "\n",
        "  # Save the model\n",
        "  '''\n",
        "  os.chdir(f'/content/drive/MyDrive/Tesi/Codice/FINAL/files/Internal_{letter}/Logistic_Regression_Hold_Out/')\n",
        "\n",
        "  pickle_file = f\"{dataset_name}_Internal_{letter}_{number}_layer.pkl\"\n",
        "  with open(pickle_file, 'wb') as file:\n",
        "      pickle.dump(model, file)\n",
        "      print(\"File saved!\")\n",
        "  '''\n",
        "  return h_states_x, labels_y, test_y, pred_y, test_ids, model, ids   #Comparison zero-shot prompt / Internal\n",
        "  #return"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "hold_out_classifier(16, \"A\", \"truthful_qa\", False)"
      ],
      "metadata": {
        "id": "NYf108TN8_s7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#T-test"
      ],
      "metadata": {
        "id": "cvP7lFEm4F59"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy.stats import ttest_rel\n",
        "#SHUFFLE AND SPLIT INDICES IN 10 FOLDS (version for samples != //10)\n",
        "# Assuming indices is an array of indices corresponding to your 1075 samples\n",
        "indices = np.arange(len(y_true))\n",
        "\n",
        "# Calculate the number of samples per fold\n",
        "samples_per_fold = len(indices) // 10  # Integer division gives number of samples per fold\n",
        "remainder = len(indices) % 10  # Calculate remainder for handling extra samples\n",
        "\n",
        "#mix indices\n",
        "np.random.shuffle(indices)\n",
        "\n",
        "# Initialize list to store split indices\n",
        "indices_splits = []\n",
        "\n",
        "# Iterate over 10 folds\n",
        "start_idx = 0\n",
        "for fold in range(10):\n",
        "    # Determine the number of samples in this fold\n",
        "    fold_size = samples_per_fold + 1 if fold < remainder else samples_per_fold\n",
        "\n",
        "    # Get indices for this fold\n",
        "    fold_indices = indices[start_idx:start_idx + fold_size]\n",
        "\n",
        "    # Store indices in list\n",
        "    indices_splits.append(fold_indices)\n",
        "\n",
        "    # Update starting index for next fold\n",
        "    start_idx += fold_size\n",
        "\n",
        "# Print out the size of each fold for verification\n",
        "for fold_idx, fold_indices in enumerate(indices_splits):\n",
        "    print(f\"Fold {fold_idx + 1}: Number of samples = {len(fold_indices)}\")\n",
        "\n",
        "y_true_binary = np.array(y_true).astype(int)\n",
        "A_pred_y_binary = np.array(A_pred_y).astype(int)\n",
        "acc_model_A = np.array([accuracy_score(y_true_binary[idxs], A_pred_y_binary[idxs]) for idxs in indices_splits])\n",
        "acc_model_A\n",
        "\n",
        "y_true_binary = np.array(y_true).astype(int)\n",
        "C_pred_y_binary = np.array(C_pred_y).astype(int)\n",
        "acc_model_C = np.array([accuracy_score(y_true_binary[idxs], C_pred_y_binary[idxs]) for idxs in indices_splits])\n",
        "acc_model_C\n",
        "\n",
        "\n",
        "t_stat, p_value = ttest_rel(acc_model_A, acc_model_C)\n",
        "# Interpretation of t-test results\n",
        "print(\"t-statistic:\", t_stat)\n",
        "print(\"p-value:\", p_value)\n",
        "alpha = 0.05\n",
        "if p_value < alpha:\n",
        "    print(\"Reject the null hypothesis: There is a significant difference between the two models.\")\n",
        "else:\n",
        "    print(\"Fail to reject the null hypothesis: There is no significant difference between the two models.\")\n",
        "\n"
      ],
      "metadata": {
        "id": "yWeRvqw04Hs8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#PCA"
      ],
      "metadata": {
        "id": "Fj17l9O_3pwM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import pandas as pd\n",
        "from sklearn.metrics import accuracy_score\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn import datasets\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import pickle\n",
        "import numpy as np\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "\n",
        "def PCA_classifier(number, letter, dataset_name, filtered):\n",
        "  os.chdir(f'/content/drive/MyDrive/Tesi/Codice/FINAL/files/Internal_{letter}/')\n",
        "  fileDataName = f\"Internal_{letter}_layer_{number}_th.jsonl\"\n",
        "  df = pd.read_json(fileDataName, lines=True)\n",
        "\n",
        "  if filtered:\n",
        "    os.chdir(f'/content/drive/MyDrive/Tesi/Codice/FINAL/files/')\n",
        "    df2 = pd.read_json('checkworthy_samples_dataset_WITH_EV.jsonl', lines=True)\n",
        "    ids = df2['ID']\n",
        "    #get the subset of the dataframe\n",
        "    filtered_df = df[df['ID'].isin(list(ids))]\n",
        "    df = filtered_df\n",
        "\n",
        "  #load data\n",
        "  h_states_x = df['hidden_states'].tolist()\n",
        "  labels_y = df[\"Factuality_ground_label\"].tolist()\n",
        "  ids = df[\"ID\"].tolist()  # Assuming the ID field is\n",
        "  #standardize the data\n",
        "  X_scaled = StandardScaler().fit_transform(h_states_x)\n",
        "\n",
        "  # Step 3: Perform SVD\n",
        "  U, S, Vt = np.linalg.svd(X_scaled, full_matrices=False)\n",
        "  '''\n",
        "  print(\"Shape U: \", U.shape)\n",
        "  print(\"Shape S: \", S.shape)\n",
        "  print(\"Shape Vt: \", Vt.shape)\n",
        "  '''\n",
        "  # Step 4: Compute explained variance and cumulative explained variance\n",
        "  explained_variance = (S ** 2) / (len(X_scaled) - 1)\n",
        "  explained_variance_ratio = explained_variance / np.sum(explained_variance)\n",
        "  cumulative_explained_variance_ratio = np.cumsum(explained_variance_ratio)\n",
        "\n",
        "  #calculate number of components\n",
        "\n",
        "  #plot\n",
        "  '''\n",
        "  xint = range(1, len(cumulative_explained_variance_ratio) + 1)\n",
        "  plt.plot(xint, cumulative_explained_variance_ratio)\n",
        "  plt.axhline(y=0.95, color='r', linestyle='--')\n",
        "  plt.text(1, 0.95, '0.95', color='black', va='center', ha='right')\n",
        "  plt.xlabel(\"Number of components\")\n",
        "  plt.ylabel(\"Cumulative explained variance\")\n",
        "  plt.xticks(xint)\n",
        "  plt.xlim(1, 4096)\n",
        "  plt.xticks(range(1, 4097, 500))  # sets ticks from 1 to 4096 with a step of 100\n",
        "  plt.grid(True)\n",
        "  '''\n",
        "\n",
        "  threshold = 0.95\n",
        "  num_components_threshold = np.argmax(cumulative_explained_variance_ratio >= threshold) + 1\n",
        "  print(f\"Number of components for {threshold*100}% variance: {num_components_threshold}\")\n",
        "\n",
        "  # Step 5: Choose number of components (optional)\n",
        "  n_components = num_components_threshold  # Example: Selecting 2 principal components\n",
        "\n",
        "  # Step 6: Project data onto principal components\n",
        "  X_pca = np.dot(X_scaled, Vt[:n_components].T)\n",
        "\n",
        "    # Split the data into training and testing sets\n",
        "  X_train, X_test, y_train, y_test, train_ids, test_ids = train_test_split(X_pca, labels_y, ids, test_size=0.2, random_state=2307)\n",
        "  name = f\"{dataset_name}_Internal_{letter}_{number}_layer\"\n",
        "  print(name.upper())\n",
        "  print(\"Train set size:\", len(X_train))\n",
        "  print(\"Test set size:\", len(X_test))\n",
        "\n",
        "  #fit model and predict\n",
        "  model = LogisticRegression(solver='lbfgs', max_iter=3000).fit(X_train, y_train)\n",
        "  pred_y = model.predict(X_test)\n",
        "  #accuracy\n",
        "  print(f'accuracy: {accuracy_score(y_test, pred_y):.4f}')\n",
        "  #confusion matrix\n",
        "  cm = confusion_matrix(y_test, pred_y, labels=[True, False])\n",
        "  disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[True, False])\n",
        "  disp.plot()\n",
        "  plt.show()\n",
        "  #classification report\n",
        "  print('\\nclassification report:\\n')\n",
        "  print(classification_report(y_test, pred_y, target_names=[\"True\",\"False\"]))\n",
        "\n",
        "  print(\"\\n\\n\\n\\n\\n\")\n",
        "  xint = range(1, len(cumulative_explained_variance_ratio) + 1)\n",
        "  plt.plot(xint, cumulative_explained_variance_ratio)\n",
        "  plt.axhline(y=0.95, color='r', linestyle='--')\n",
        "  plt.text(1, 0.95, '0.95', color='black', va='center', ha='right')\n",
        "  plt.xlabel(\"Number of components\")\n",
        "  plt.ylabel(\"Cumulative explained variance\")\n",
        "  plt.xticks(xint)\n",
        "  plt.xlim(1, 4096)\n",
        "  plt.xticks(range(1, 4097, 500))  # sets ticks from 1 to 4096 with a step of 100\n",
        "  plt.grid(True)\n",
        "\n",
        "\n",
        "PCA_classifier(32, \"C\", \"checkworthy_dataset_ev\", True)"
      ],
      "metadata": {
        "id": "V7GR-xG53sIy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CJMMzMh_0xdV"
      },
      "source": [
        "# Review of the checkworthiness labellings from the test sets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IAvmqzRH7hpu"
      },
      "outputs": [],
      "source": [
        "os.chdir('/content/drive/MyDrive/Tesi/Codice/FINAL/files')\n",
        "df = pd.read_json('16_C(old_check_labels).jsonl', lines=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Insert here the group you want to analyze (TP, TN, FP, FN)"
      ],
      "metadata": {
        "id": "khBh8x1YBnlN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GyWHgfpG9S8R"
      },
      "outputs": [],
      "source": [
        "check_df = df[(df['Model_checkworthiness_label'] == False) &\n",
        "                 (df['Predicted'] == False) &\n",
        "                 (df['Actual'] == True)]\n",
        "len(check_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I31HuyAzNsCm"
      },
      "outputs": [],
      "source": [
        "check_df[\"Category\"].unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t97eH8ObN2d1"
      },
      "outputs": [],
      "source": [
        "print(\"CHECKWORTHY: true, PRED: true, REAL: true\")\n",
        "check_df['Category'].value_counts()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a96RQLQXOUPb"
      },
      "outputs": [],
      "source": [
        "check_df[check_df['Category'] == 'Paranormal']\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.chdir('/content/drive/MyDrive/Tesi/Codice/FINAL/files')\n",
        "df = pd.read_json('NEW_manually_reviewed_checkworthy_test_set.jsonl', lines=True)"
      ],
      "metadata": {
        "id": "LI52h-duCARC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(df[(df['Model_checkworthiness_label'] == False)])"
      ],
      "metadata": {
        "id": "JFx2tGb5CH9b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NDnGrDlQUwJo"
      },
      "source": [
        "#EV_Zero-shot verify prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tZoYyIfpVB70"
      },
      "outputs": [],
      "source": [
        "os.chdir('/content/drive/MyDrive/Tesi/Codice/FINAL/src')\n",
        "!python EV_hal_detect_prompt.py --input 'SHUFFLED_WITH_EV_truthful_qa_EXCLUDE_NO_EV.jsonl' --output 'EV_truthful_qa_zero_shot_prompt.jsonl'  --prompt 'verifyPrompt.txt' --resume"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pwm50boj8Zqa"
      },
      "source": [
        "#NO_EV Zero-shot verify prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1rSOFGKp8mWJ"
      },
      "outputs": [],
      "source": [
        "os.chdir('/content/drive/MyDrive/Tesi/Codice/FINAL/src')\n",
        "!python NO_EV_hal_detect_prompt.py --input 'SHUFFLED_WITH_EV_truthful_qa_FILTER_NO_EV.jsonl' --output 'NO_EV_truthful_qa_zero_shot_prompt.jsonl'  --prompt 'NO_EV_verifyPrompt.txt' --resume"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GjzO2IkejVKU"
      },
      "source": [
        "#Comparison zero-shot prompt / Internal_16\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluate zero shot with evidence on complete dataset"
      ],
      "metadata": {
        "id": "gvWlUg2fLcKR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "#EV_zero_shot complete\n",
        "os.chdir(f'/content/drive/MyDrive/Tesi/Codice/FINAL/files/Zero-shot')\n",
        "fileDataName = \"EV_truthful_qa_zero_shot_prompt.jsonl\"\n",
        "df = pd.read_json(fileDataName, lines=True)\n",
        "#Model_factuality_label != NaN\n",
        "df_cleaned = df.dropna(subset=['Model_factuality_label'])\n",
        "z_real = df_cleaned['Factuality_ground_label']\n",
        "z_pred = df_cleaned['Model_factuality_label']\n",
        "#accuracy internal\n",
        "print(\"TRUTHFUL_QA_EV\")\n",
        "print(f'accuracy zero_shot EV: {accuracy_score(z_real, z_pred ):.4f}')\n",
        "print(classification_report(z_real, z_pred , target_names=[\"True\",\"False\"]))\n",
        "\n",
        "\n",
        "#confusion matrix\n",
        "cm = confusion_matrix(z_real, z_pred, labels=[True, False])\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[True, False])\n",
        "disp.plot()\n",
        "plt.savefig(\"EV_zero_shot_complete.pdf\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Rcc8d5dZie8Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluate zero shot with no evidence on complete dataset (except for the samples which were null in zero_shot_ev)"
      ],
      "metadata": {
        "id": "EtDSMLQKLhub"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "from sklearn.metrics import classification_report\n",
        "#from EV\n",
        "df_clean_ev = df_cleaned\n",
        "#NO_EV_zero_shot complete\n",
        "os.chdir(f'/content/drive/MyDrive/Tesi/Codice/FINAL/files/Zero-shot')\n",
        "fileDataName = \"NO_EV_truthful_qa_zero_shot_prompt.jsonl\"\n",
        "df = pd.read_json(fileDataName, lines=True)\n",
        "print(\"Len complete \", len(df))\n",
        "#Model_factuality_label != NaN\n",
        "df_cleaned = df.dropna(subset=['Model_factuality_label'])\n",
        "df_cleaned = df_cleaned[df_cleaned['ID'].isin(df_clean_ev['ID'])]\n",
        "print(\"Len not NaN \", len(df_cleaned))\n",
        "z_real = df_cleaned['Factuality_ground_label']\n",
        "z_pred = df_cleaned['Model_factuality_label']\n",
        "#accuracy internal\n",
        "print(\"TRUTHFUL_QA_EV\")\n",
        "print(f'accuracy zero_shot EV: {accuracy_score(z_real, z_pred ):.4f}')\n",
        "print(classification_report(z_real, z_pred , target_names=[\"True\",\"False\"]))\n",
        "\n",
        "\n",
        "#confusion matrix\n",
        "cm = confusion_matrix(z_real, z_pred, labels=[True, False])\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[True, False])\n",
        "disp.plot()\n",
        "plt.savefig(\"NO_EV_zero_shot_complete.pdf\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "U8Pyck5bjzin"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KJXTvFZu2Pxz"
      },
      "source": [
        "Internal evidence_question_answer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rWDjv3Dxjf0s"
      },
      "outputs": [],
      "source": [
        "#go in Hold-Out + Logistic Regression to get the function\n",
        "X, y, C_test_y, C_pred_y , C_test_ids, C_model, C_ids  = hold_out_classifier(16, \"C\", \"truthful_qa_EV\", False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3CL-k5gRjmsW"
      },
      "outputs": [],
      "source": [
        "df_internal_C = pd.DataFrame({\n",
        "    'ID': C_test_ids,\n",
        "    'Actual': C_test_y,\n",
        "    'Predicted': C_pred_y\n",
        "})\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "EV_ZERO_SHOT EVALUATED ON TEST SET\n"
      ],
      "metadata": {
        "id": "NSrWCzvDLwm1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w_TReswRjwUh"
      },
      "outputs": [],
      "source": [
        "#take from zero_shot\n",
        "os.chdir(f'/content/drive/MyDrive/Tesi/Codice/FINAL/files/Zero-shot')\n",
        "fileDataName = \"EV_truthful_qa_zero_shot_prompt.jsonl\"\n",
        "df = pd.read_json(fileDataName, lines=True)\n",
        "#leave only the ids from the test set\n",
        "filtered_df = df[df['ID'].isin(C_test_ids)]\n",
        "#take only the value that have model_label != null. (model_label = null: 20(Complete) or 16(checkworthy))\n",
        "good_zero_shot_df = filtered_df[filtered_df['Model_factuality_label'].notna()]\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "real = df_internal_C['Actual']\n",
        "pred = df_internal_C['Predicted']\n",
        "#accuracy internal\n",
        "print(\"TRUTHFUL_QA_EV\")\n",
        "print(f'accuracy internal C: {accuracy_score(real, pred ):.4f}')\n",
        "print(classification_report(real, pred , target_names=[\"True\",\"False\"]))\n",
        "\n",
        "#confusion matrix\n",
        "cm = confusion_matrix(real, pred, labels=[True, False])\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[True, False])\n",
        "disp.plot()\n",
        "plt.show()\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "z_real = good_zero_shot_df['Factuality_ground_label']\n",
        "z_pred = good_zero_shot_df['Model_factuality_label']\n",
        "#accuracy internal\n",
        "print(\"TRUTHFUL_QA_EV\")\n",
        "print(f'accuracy zero_shot EV: {accuracy_score(z_real, z_pred ):.4f}')\n",
        "print(classification_report(z_real, z_pred , target_names=[\"True\",\"False\"]))\n",
        "\n",
        "#confusion matrix\n",
        "cm = confusion_matrix(z_real, z_pred, labels=[True, False])\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[True, False])\n",
        "disp.plot()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "NO_EV_ZERO_SHOT EVALUATED ON TEST SET (except for the samples which were null in ev_zero_shot)\n"
      ],
      "metadata": {
        "id": "B9r0JNXfL2U4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#take from zero_shot\n",
        "os.chdir(f'/content/drive/MyDrive/Tesi/Codice/FINAL/files/Zero-shot')\n",
        "fileDataName = \"NO_EV_truthful_qa_zero_shot_prompt.jsonl\"\n",
        "df = pd.read_json(fileDataName, lines=True)\n",
        "#leave only the ids from the test set\n",
        "filtered_df = df[df['ID'].isin(good_zero_shot_df['ID'])]\n",
        "\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "z_real = filtered_df['Factuality_ground_label']\n",
        "z_pred = filtered_df['Model_factuality_label']\n",
        "#accuracy internal\n",
        "print(\"TRUTHFUL_QA_EV\")\n",
        "print(f'accuracy zero_shot EV: {accuracy_score(z_real, z_pred ):.4f}')\n",
        "print(classification_report(z_real, z_pred , target_names=[\"True\",\"False\"]))\n",
        "\n",
        "#confusion matrix\n",
        "cm = confusion_matrix(z_real, z_pred, labels=[True, False])\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[True, False])\n",
        "disp.plot()\n",
        "plt.savefig('test_no_ev_zero_shot_prompt.pdf')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Yhf1NyrrL6zs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Analysis of the solutions' performance in classifying non-checkworthy samples"
      ],
      "metadata": {
        "id": "aycmErm1HkmQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##NON_CHECKWORTHY_NO_EV_Zero_shot_prompt"
      ],
      "metadata": {
        "id": "LsbIXcR6O4Yr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.chdir('/content/drive/MyDrive/Tesi/Codice/FINAL/files')\n",
        "title = \"NON_CHECKWORTHY_TEST_NO_EV_Zero_shot_prompt\"\n",
        "df = pd.read_json('NO_EV_NEW_CHECK_LABELS_TEST_truthful_qa_zero_shot_prompt.jsonl', lines = True)\n",
        "df = df[df['Model_factuality_label'].notna()]   #Take away the samples that the model could not classify (In this case, no samples are deleted)\n",
        "len(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g5XlMK1PKeC7",
        "outputId": "9c47130b-fac9-4cd9-d4f8-cac5b3bd7be6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1075"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#analyze only the non-checkworthy samples\n",
        "df = df[df['Model_checkworthiness_label'] == False]\n",
        "len(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JAIxtFxkLAzj",
        "outputId": "0f764b43-b5e5-4467-d6fd-e75f78a8639b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "112"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns"
      ],
      "metadata": {
        "id": "tW6LCgNpN9TD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a9286fa3-2036-4bb1-e2a9-e443644624a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['ID', 'Type', 'Category', 'Question', 'Answer', 'Source',\n",
              "       'Factuality_ground_label', 'Model_factuality_judgement',\n",
              "       'Model_factuality_label', 'Model_checkworthiness_label'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##NON_CHECKWORTHY_EV_Zero_shot_prompt"
      ],
      "metadata": {
        "id": "yJR9jHXFPgdX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.chdir('/content/drive/MyDrive/Tesi/Codice/FINAL/files')\n",
        "title = \"NON_CHECKWORTHY_TEST_EV_Zero_shot_prompt\"\n",
        "df = pd.read_json('EV_NEW_CHECK_LABELS_TEST_truthful_qa_zero_shot_prompt.jsonl', lines = True)\n",
        "df = df[df['Model_factuality_label'].notna()]   #Take away the samples that the model could not classify\n",
        "len(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CtFo8W9xPm_Y",
        "outputId": "377c3445-b20c-406f-fc12-7ea87924a567"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1055"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#analyze only the non-checkworthy samples\n",
        "df = df[df['Model_checkworthiness_label'] == False]\n",
        "len(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T98EW_llPqb6",
        "outputId": "faefe074-f95a-4089-976b-a0072874afea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "109"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##NON_CHECKWORTHY_TEST_EV_Internal"
      ],
      "metadata": {
        "id": "WytKc0NyQlaR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.chdir('/content/drive/MyDrive/Tesi/Codice/FINAL/files')\n",
        "title = \"NON_CHECKWORTHY_TEST_EV_Internal\"\n",
        "df = pd.read_json('NEW_CHECK_LABELS_TEST_truthful_qa_Internal_C.jsonl', lines = True)\n",
        "len(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "790a32de-7281-4912-fa2b-9886cb1e038d",
        "id": "ktUJgklfQlad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1075"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#analyze only the non-checkworthy samples\n",
        "df = df[df['Model_checkworthiness_label'] == False]\n",
        "len(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "43e8083f-65aa-4827-fdda-e67a5c991d89",
        "id": "9YSRHl3NQlae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "112"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MLqVrn8ORS6m",
        "outputId": "0eb27ac1-a446-476f-e758-418cc9d9c5aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['ID', 'Predicted', 'Actual', 'Model_checkworthiness_label'], dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##NON_CHECKWORTHY_TEST_NO_EV_Internal"
      ],
      "metadata": {
        "id": "CRmmnq2USNSn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.chdir('/content/drive/MyDrive/Tesi/Codice/FINAL/files')\n",
        "title = \"NON_CHECKWORTHY_TEST_NO_EV_Internal\"\n",
        "df = pd.read_json('NEW_CHECK_LABELS_TEST_truthful_qa_Internal_A.jsonl', lines = True)\n",
        "len(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d068f37c-fb6a-4c21-e349-f261069c0a5d",
        "id": "oKjFjbfHSNSz"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1075"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#analyze only the non-checkworthy samples\n",
        "df = df[df['Model_checkworthiness_label'] == False]\n",
        "len(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "50fd4407-e44f-45b6-b72d-fceb355642b3",
        "id": "emfhhoyZSNS0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "112"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c54ce800-d319-428d-ba2c-f8690596478d",
        "id": "m2m429HtSNS0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['ID', 'Predicted', 'Actual', 'Model_checkworthiness_label'], dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##FindPerformance"
      ],
      "metadata": {
        "id": "deODkp_oQA42"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "def findPerformance(df, title):\n",
        "  #Zero-shot files\n",
        "  true = df['Factuality_ground_label']\n",
        "  pred = df['Model_factuality_label']\n",
        "\n",
        "  #Internal files\n",
        "  #true = df[\"Actual\"]\n",
        "  #pred = df['Predicted']\n",
        "\n",
        "  true = true.astype(int)\n",
        "  print(title)\n",
        "  #accuracy\n",
        "  print(f'accuracy: {accuracy_score(true, pred):.4f}')\n",
        "  #confusion matrix\n",
        "  cm = confusion_matrix(true, pred, labels=[True, False])\n",
        "  disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[True, False])\n",
        "  disp.plot()\n",
        "  plt.savefig(title + \".pdf\")\n",
        "  plt.show()\n",
        "  #classification report\n",
        "  print('\\nclassification report:\\n')\n",
        "  print(classification_report(true, pred, target_names=[\"True\",\"False\"]))\n",
        "\n",
        "findPerformance(df, title)"
      ],
      "metadata": {
        "id": "YVnyP9DuLmey"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "L3e2RZ5Vcimd"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "zIKjwn71V3yq",
        "C5Mp638pTlGE",
        "BdAvMZt8UPqq",
        "w278fYbgl5ug",
        "qaIGNXQ-XQaV",
        "xukiUAe5gaEh",
        "vVarnVnx_X3j",
        "YRgiHsw7gAEZ",
        "4P7dJUUyQScg",
        "cvP7lFEm4F59",
        "Fj17l9O_3pwM",
        "CJMMzMh_0xdV",
        "NDnGrDlQUwJo",
        "pwm50boj8Zqa",
        "LsbIXcR6O4Yr",
        "yJR9jHXFPgdX",
        "CRmmnq2USNSn"
      ],
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}